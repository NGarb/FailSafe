{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96ff5b9f-43a2-4663-8b38-2cdeffe067fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "access_df = pd.read_csv('./data/train.csv')\n",
    "temp_dir = 'D:/Projects/FailSafe_500/11. AmazonAccess/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b222b-f314-4e86-a8b0-a01f54b64d82",
   "metadata": {},
   "source": [
    "Note: not cloning the entire repo because would like to work through all the code and make sense of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f46d3a8-12e2-4d87-94bb-e32abaab8790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initially ignoring all the config stuff and parsing through main although that is something new and v cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fc68077-4772-4698-beda-84037325dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from\n",
    "Author: Paul Duan \n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "from sklearn import metrics, linear_model, ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ml\n",
    "import diagnostics\n",
    "from data import load_data, save_results\n",
    "from feature_extraction import create_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38bf45a1-884b-4c35-8a62-61433ffbb3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.basicConfig(format=\"[%(asctime)s] %(levelname)s\\t%(message)s\",\n",
    "                    filename=\"history.log\", filemode='a', level=logging.DEBUG,\n",
    "                    datefmt='%m/%d/%y %H:%M:%S')\n",
    "formatter = logging.Formatter(\"[%(asctime)s] %(levelname)s\\t%(message)s\",\n",
    "                              datefmt='%m/%d/%y %H:%M:%S')\n",
    "console = logging.StreamHandler()\n",
    "console.setFormatter(formatter)\n",
    "console.setLevel(logging.INFO)\n",
    "logging.getLogger().addHandler(console)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9db0efb2-520b-4afc-9319-abc4e0d509b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \"\"\"\n",
    "    The final model is a combination of several base models, which are then\n",
    "    combined using StackedClassifier defined in the helpers.ml module.\n",
    "\n",
    "    The list of models and associated datasets is generated automatically\n",
    "    from their identifying strings. The format is as follows:\n",
    "    A:b_c where A is the initials of the algorithm to use, b is the base\n",
    "    dataset, and c is the feature set and the variants to use.\n",
    "    \"\"\"\n",
    "SEED = 42\n",
    "selected_models = [\n",
    "    \"LR:tuples_sf\",\n",
    "    \"LR:greedy_sfl\",\n",
    "    \"LR:greedy2_sfl\",\n",
    "    \"LR:greedy3_sf\",\n",
    "    \"RFC:basic_b\",\n",
    "    \"RFC:tuples_f\",\n",
    "    \"RFC:tuples_fd\",\n",
    "    \"RFC:greedy_f\",\n",
    "    \"RFC:greedy2_f\",\n",
    "    \"GBC:basic_f\",\n",
    "    \"GBC:tuples_f\",\n",
    "    \"LR:greedy_sbl\",\n",
    "    \"GBC:greedy_c\",\n",
    "    \"GBC:tuples_cf\",\n",
    "    #\"RFC:effects_f\",  # experimental; added after the competition\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f53d6863-fb05-4be2-862f-99e66327d3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03/01/22 09:57:35] INFO\tloading data\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"loading data\")\n",
    "\n",
    "y = pd.read_csv(temp_dir+'train.csv')['ACTION']\n",
    "X = pd.read_csv(temp_dir+'train.csv').drop(['ACTION'],axis=1)\n",
    "X_test = pd.read_csv(temp_dir+'test.csv').drop(['id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14d95cd1-61af-44b5-b840-7b3e80dd673c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id, dataset = selected_models[0].split(':')\n",
    "{'LR': linear_model.LogisticRegression}['LR']().set_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28dec0ea-2a14-4e2f-b3b8-620d02fdc694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03/01/22 09:57:36] INFO\tpreparing datasets (use_cache=%s)\n"
     ]
    }
   ],
   "source": [
    "# Create the models on the fly\n",
    "models = []\n",
    "for item in selected_models:\n",
    "    model_id, dataset = item.split(':')\n",
    "    # instantiating models (with datasets)\n",
    "    model = {'LR': linear_model.LogisticRegression,\n",
    "             'GBC': ensemble.GradientBoostingClassifier,\n",
    "             'RFC': ensemble.RandomForestClassifier,\n",
    "             'ETC': ensemble.ExtraTreesClassifier}[model_id]() #I have never seen this done before.\n",
    "    model.set_params(random_state=SEED)\n",
    "    models.append((model, dataset))\n",
    "\n",
    "datasets = [dataset for model, dataset in models]\n",
    "\n",
    "logger.info(\"preparing datasets (use_cache=%s)\")#, str(CONFIG.use_cache))\n",
    "create_datasets(X, X_test, y, datasets)#, CONFIG.use_cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "283d057c-afa8-4c31-a08f-a3eec738897f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[03/01/22 10:15:29] INFO\tfound params (LR:tuples_sf > 0.8971): {'C': 1.5, 'class_weight': 'auto'}\n",
      "C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[03/01/22 10:25:59] INFO\tfound params (LR:greedy_sfl > 0.9069): {'C': 1.5, 'class_weight': 'auto'}\n",
      "C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[03/01/22 10:35:24] INFO\tfound params (LR:greedy2_sfl > 0.9070): {'C': 1.5, 'class_weight': 'auto'}\n",
      "C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[03/01/22 10:40:18] INFO\tfound params (LR:greedy3_sf > 0.9081): {'C': 2, 'class_weight': 'auto'}\n",
      "C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "240 fits failed out of a total of 960.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\joblib\\parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.88775731 0.88911943 0.88862273        nan 0.8773962\n",
      " 0.8781021  0.87652926        nan 0.86969839 0.86970981 0.86889815\n",
      "        nan 0.85068991 0.85039272 0.84930237        nan 0.8911337\n",
      " 0.89159587 0.89156041        nan 0.88100072 0.88060589 0.88063436\n",
      "        nan 0.87279519 0.87460634 0.87382489        nan 0.85581231\n",
      " 0.85556446 0.85629067        nan 0.89156615 0.89097781 0.89203375\n",
      "        nan 0.8803882  0.88064043 0.88137799        nan 0.87431213\n",
      " 0.87440959 0.87444461        nan 0.85703133 0.85799669 0.85638393\n",
      "        nan 0.89023998 0.8918347  0.8916402         nan 0.88076512\n",
      " 0.87979889 0.88206439        nan 0.87542936 0.87373105 0.87467536\n",
      "        nan 0.8575638  0.85791706 0.85678425        nan 0.88991739\n",
      " 0.89212598 0.8913041         nan 0.88065612 0.87997416 0.88205605\n",
      "        nan 0.87523901 0.87391202 0.87459504        nan 0.85780974\n",
      " 0.85826811 0.85687096        nan 0.89006793 0.89207053 0.89129846\n",
      "        nan 0.88059393 0.87998954 0.88197296        nan 0.87526561\n",
      " 0.87388814 0.87465125        nan 0.85785512 0.85826742 0.8568816 ]\n",
      "  warnings.warn(\n",
      "[03/01/22 16:04:47] INFO\tfound params (RFC:basic_b > 0.8921): {'max_depth': 35, 'max_features': 3, 'min_samples_split': 5, 'n_jobs': 1}\n",
      "C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "240 fits failed out of a total of 960.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\joblib\\parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.89994105 0.89998164 0.89927046        nan 0.89696502\n",
      " 0.89583267 0.89579926        nan 0.89488237 0.89347025 0.89373698\n",
      "        nan 0.89007811 0.89010076 0.88901814        nan 0.90061629\n",
      " 0.90057499 0.90030869        nan 0.89691655 0.8966993  0.89653777\n",
      "        nan 0.89443154 0.89441165 0.89414607        nan 0.89158635\n",
      " 0.89239774 0.89185191        nan 0.89974432 0.90109873 0.90023568\n",
      "        nan 0.89724127 0.89780192 0.8976059         nan 0.89449145\n",
      " 0.89461088 0.89523643        nan 0.89283804 0.89254683 0.89119481\n",
      "        nan 0.90049117 0.90094697 0.9001972         nan 0.89720828\n",
      " 0.89744429 0.89712898        nan 0.89421858 0.8943418  0.8948911\n",
      "        nan 0.89257621 0.89312982 0.89102002        nan 0.90017994\n",
      " 0.90110347 0.90025792        nan 0.89694998 0.89717262 0.89736566\n",
      "        nan 0.89410483 0.89481383 0.89465467        nan 0.89266953\n",
      " 0.89280545 0.89128967        nan 0.90015091 0.90112369 0.90029474\n",
      "        nan 0.89685446 0.89714614 0.89738003        nan 0.89404267\n",
      " 0.89479379 0.89459787        nan 0.89272846 0.89285305 0.89129895]\n",
      "  warnings.warn(\n",
      "[03/01/22 21:27:05] INFO\tfound params (RFC:tuples_f > 0.9011): {'max_depth': None, 'max_features': 3, 'min_samples_split': 5, 'n_jobs': 1}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27804/3235230209.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Set params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_set\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m clf = ml.StackedClassifier(\n\u001b[0;32m      5\u001b[0m     models)#, stack=CONFIG.stack, fwls=CONFIG.fwls)#,model_selection=CONFIG.model_selection,use_cached_models=CONFIG.use_cache)\n",
      "\u001b[1;32mD:\\Projects\\FailSafe_500\\11. AmazonAccess\\ml.py\u001b[0m in \u001b[0;36mfind_params\u001b[1;34m(model, feature_set, y, subsample, grid_search)\u001b[0m\n\u001b[0;32m    370\u001b[0m         clf = GridSearchCV(model, PARAM_GRID[model_name], cv=10, n_jobs=6,\n\u001b[0;32m    371\u001b[0m                            scoring=\"roc_auc\")\n\u001b[1;32m--> 372\u001b[1;33m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    373\u001b[0m         logger.info(\"found params (%s > %.4f): %s\",\n\u001b[0;32m    374\u001b[0m                     \u001b[0mstringify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    889\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1390\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1392\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    836\u001b[0m                     )\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m                 out = parallel(\n\u001b[0m\u001b[0;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[0;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AutoML\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AutoML\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    936\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AutoML\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AutoML\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    438\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AutoML\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set params\n",
    "for model, feature_set in models:\n",
    "    model.set_params(**ml.find_params(model, feature_set, y,grid_search=True))\n",
    "clf = ml.StackedClassifier(\n",
    "    models)#, stack=CONFIG.stack, fwls=CONFIG.fwls)#,model_selection=CONFIG.model_selection,use_cached_models=CONFIG.use_cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ad03953-74e0-40a5-ab10-b9983239d5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tuples_sf',\n",
       " 'greedy_sfl',\n",
       " 'greedy2_sfl',\n",
       " 'greedy3_sf',\n",
       " 'basic_b',\n",
       " 'tuples_f',\n",
       " 'tuples_fd',\n",
       " 'greedy_f',\n",
       " 'greedy2_f',\n",
       " 'basic_f',\n",
       " 'tuples_f',\n",
       " 'greedy_sbl',\n",
       " 'greedy_c',\n",
       " 'tuples_cf']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c83c6593-e294-4e35-870a-4b1dcf6fef3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03/02/22 13:58:15] INFO\tcomputing cv score\n",
      "C:\\Users\\nikig\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cache/models/main/LR:tuples_sf15794.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27804/2735435116.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     train, cv = train_test_split(\n\u001b[0;32m      6\u001b[0m         range(len(y)), test_size=.20, random_state=1+i*SEED)\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mcv_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, show_steps=CONFIG.verbose)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mfpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv_preds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Projects\\FailSafe_500\\11. AmazonAccess\\ml.py\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, y, train, predict, show_steps)\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[0mcache_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstringify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_set\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m             model_preds = self._get_model_preds(\n\u001b[0m\u001b[0;32m    293\u001b[0m                 model, X_train, X_predict, y_train, cache_file)\n\u001b[0;32m    294\u001b[0m             \u001b[0mstage0_predict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_preds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Projects\\FailSafe_500\\11. AmazonAccess\\ml.py\u001b[0m in \u001b[0;36m_get_model_preds\u001b[1;34m(self, model, X_train, X_predict, y_train, cache_file)\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m             \u001b[0mmodel_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m             with open(\"cache/models/%s/%s.pkl\" % (\n\u001b[0m\u001b[0;32m    229\u001b[0m                     self.cache_dir, cache_file), 'wb') as f:\n\u001b[0;32m    230\u001b[0m                 \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_preds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cache/models/main/LR:tuples_sf15794.pkl'"
     ]
    }
   ],
   "source": [
    "#  Metrics\n",
    "logger.info(\"computing cv score\")\n",
    "mean_auc = 0.0\n",
    "for i in range(100):\n",
    "    train, cv = train_test_split(range(len(y)), test_size=.20, random_state=1+i*SEED)\n",
    "    cv_preds = clf.fit_predict(y, train, cv)#, show_steps=CONFIG.verbose)\n",
    "\n",
    "    fpr, tpr, _ = metrics.roc_curve(y[cv], cv_preds)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    logger.info(\"AUC (fold %d/%d): %.5f\", i + 1, 100, roc_auc)\n",
    "    mean_auc += roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1331a5b-0a38-40b1-8db6-d4ff9514942a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c6288f-a09a-43c3-8142-f3c0e175bdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "if CONFIG.diagnostics and i == 0:  # only plot for first fold\n",
    "        logger.info(\"plotting learning curve\")\n",
    "        diagnostics.learning_curve(clf, y, train, cv)\n",
    "        diagnostics.plot_roc(fpr, tpr)\n",
    "if CONFIG.iter:\n",
    "    logger.info(\"Mean AUC: %.5f\",  mean_auc/CONFIG.iter)\n",
    "\n",
    "# Create submissions\n",
    "if CONFIG.outputfile:\n",
    "    logger.info(\"making test submissions (CV AUC: %.4f)\", mean_auc)\n",
    "    preds = clf.fit_predict(y, show_steps=CONFIG.verbose)\n",
    "    save_results(preds, CONFIG.outputfile + \".csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
