{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5846da79-9d1a-42ef-a904-11a36b8b7087",
   "metadata": {},
   "source": [
    "## Modeling for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d03dd50-ea4f-49d1-8a0e-b4b847f39714",
   "metadata": {},
   "source": [
    "### implementing some learnings from Amazon Access (project 11)\n",
    "- generating multiple datasets \n",
    "- zipping datasets with models \n",
    "- stacking models for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e9ba757-efd3-47e2-a50b-e2ee6f2322eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn import metrics, linear_model, ensemble\n",
    "from yellowbrick.regressor import residuals_plot, prediction_error\n",
    "from fast_ml.model_development import train_valid_test_split\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from tpot import TPOTRegressor\n",
    "import category_encoders as ce\n",
    "import time\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "encoders = {\n",
    "\n",
    "    'BackwardDifferenceEncoder': ce.backward_difference.BackwardDifferenceEncoder,\n",
    "    'OneHotEncoder': ce.one_hot.OneHotEncoder,\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dbb945a-a221-46d2-98c0-3ee2343861b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('..')\n",
    "from Utils.Metrics import regression as reg_metrics\n",
    "os.chdir('./9. Clickstream data for online shopping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f630143b-9aff-47ee-aab1-fc1a78fe4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "clothing_data_df = pd.read_csv('./data/e-shop data and description/e-shop clothing 2008.csv',sep=',').drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "be9fbfbf-cd62-4de3-b486-ba42e02bce94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "35d01c53-f86f-474c-b621-c54be684c52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d08b2e09-e858-46c2-960b-499ac035327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_models = [\n",
    "    \"RFR:dataset1\",\n",
    "    \"RFR:dataset2\",\n",
    "    \"RFR:dataset3\",\n",
    "    \"RFR:dataset4\",\n",
    "    \"RFR:dataset5\",\n",
    "    \"GBR:dataset1\",\n",
    "    \"GBR:dataset2\",\n",
    "    \"GBR:dataset3\",\n",
    "    \"GBR:dataset4\"\n",
    "    \"GBR:dataset5\",\n",
    "    \"ETR:dataset1\",\n",
    "    \"ETR:dataset2\",\n",
    "    \"ETR:dataset3\",\n",
    "    \"ETR:dataset4\",\n",
    "    \"ETR:dataset5\",\n",
    "]\n",
    "\n",
    "models = []\n",
    "for item in selected_models:\n",
    "    model_id, dataset = item.split(':')\n",
    "    # instantiating models (with datasets)\n",
    "    model = {'GBR': ensemble.GradientBoostingRegressor,\n",
    "             'RFR': ensemble.RandomForestRegressor,\n",
    "             'ETR': ensemble.ExtraTreesRegressor}[model_id]() \n",
    "    model.set_params()\n",
    "    models.append((model, dataset))\n",
    "\n",
    "datasets = [dataset for model, dataset in models]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b225a5-a5c9-4b83-a594-1057bbdbafd6",
   "metadata": {},
   "source": [
    "## Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1cdf397-623a-46ba-92bd-fe015af84246",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clothing_data_df.drop('price', axis=1)\n",
    "y = clothing_data_df['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bed3eb4-0ee2-46e6-a461-5ee76e71af19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 1\n",
    "# Create a dataset where the features are the effects of a logistic regression trained on sparsified data.\n",
    "# Essentially what this would give you is the exact effect of the value of a feature on the target. would work if target is binary but in my case there are twenty unique values\n",
    "# redefine target to <= 40 or > 40\n",
    "y = (y <= 40).astype(int)\n",
    "\n",
    "def sparsify(X, X_test):\n",
    "    \"\"\"Return One-Hot encoded datasets.\"\"\"\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(np.vstack((X, X_test)))\n",
    "    return enc.transform(X.values), enc.transform(X_test.values)\n",
    "\n",
    "from sklearn import linear_model \n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "Xe_train = np.zeros(X_train.shape)\n",
    "Xe_test = np.zeros(X_test.shape)\n",
    "n_cols = Xe_train.shape[1]\n",
    "\n",
    "model = linear_model.LogisticRegression(C=2)\n",
    "X_train, X_test = sparsify(X_train, X_test)\n",
    "\n",
    "kfold = KFold(5)\n",
    "\n",
    "# alrighty. so what's happening is unrolling of all unique values in all columns, for each of these columns, training a logistic model and then slotting those values (coeffs) back into the unrolled spaces\n",
    "for train, cv in kfold.split(X_train):\n",
    "    model.fit(X_train[train], y[train])\n",
    "    colindices = X_test.nonzero()[1] #flattened non zero col indices #remember the data has been sparsified so 14501 unique col indices and 364045 non unique\n",
    "    for i, k in zip(cv, range(len(cv))): # i - row number, k - row index\n",
    "        for j in range(n_cols): #n_cols is 11 (pre sparsify)\n",
    "            z = colindices[n_cols*k + j]\n",
    "            Xe_train[i, j] = model.coef_[0, z]\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "'''colindices = X_test.nonzero()[1]\n",
    "for i in range(Xe_test.shape[0]):\n",
    "    for j in range(n_cols):\n",
    "        z = colindices[n_cols*i + j]\n",
    "        Xe_test[i, j] = model.coef_[0, z]''' \n",
    "\n",
    "\n",
    "dataset1 = Xe_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a0f8d78-a51e-41ce-b441-97a206d2b512",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_44528/2256004681.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mXe_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/datatset1.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(Xe_train).to_csv('./data/datatset1.csv')\n",
    "pd.DataFrame(Xe_test).to_csv('./data/datatset1_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8353a7c-ab31-419c-b402-89910cf3bce7",
   "metadata": {},
   "source": [
    "## Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4b0a703-2143-44ec-ae80-0faaada40318",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clothing_data_df.drop('price', axis=1)\n",
    "y = clothing_data_df['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c786f069-0ec8-47a9-b12a-854ab07c4a9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_44528/4149570083.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "# with polynomial features \n",
    "    \n",
    "# ordinal encode \n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(np.hstack((X_train['page 2 (clothing model)'], X_test['page 2 (clothing model)'])))\n",
    "X_train['page 2 (clothing model)'] = le.transform(X_train['page 2 (clothing model)'])\n",
    "X_test['page 2 (clothing model)'] = le.transform(X_test['page 2 (clothing model)'])\n",
    "    \n",
    "X_train = list(X_train.T.values)\n",
    "X_test = list(X_test.T.values)\n",
    "X_train = [list(feature) for feature in X_train]\n",
    "X_test = [list(feature) for feature in X_test]\n",
    "\n",
    "n_features = len(X_train)    \n",
    "    \n",
    "for i in range(n_features):\n",
    "    #for j in range(1): j = 0 ? ratio of all features with first one? is it maybe meant to be range(i+1,n_features)? \n",
    "    for j in range(i+1,n_features):\n",
    "        X_train.append([round(a/(b + 1), 3) for a, b in zip(X_train[i], X_train[j])])\n",
    "        X_test.append([round(a/(b + 1), 3) for a, b in zip(X_test[i], X_test[j])])\n",
    "\n",
    "        X_train.append([round(a/(b + 1), 3) for a, b in zip(X_train[j], X_train[i])])\n",
    "        X_test.append([round(a/(b + 1), 3) for a, b in zip(X_test[j], X_test[i])])\n",
    "\n",
    "        X_train.append([a*b for a, b in zip(X_train[j], X_train[i])])\n",
    "        X_test.append([a*b for a, b in zip(X_test[j], X_test[i])])\n",
    "\n",
    "# remove constant features\n",
    "for i in range(len(X_train) - 1, -1, -1):\n",
    "    if np.var(X_train[i]) + np.var(X_test[i]) == 0:\n",
    "        X_train.pop(i)\n",
    "        X_test.pop(i)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1692757-20ee-49aa-a17f-da7ae3062290",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train).T\n",
    "X_test = pd.DataFrame(X_test).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dee5d0db-4d79-4e0e-bbdd-b873113f4ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132379, 176)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cebdc059-7135-4cf9-91bb-735f8124834e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features in the green area: [5, 6, 7, 8, 26, 27, 29, 30, 33, 34, 36, 38, 39, 103, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 142, 143, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 175]\n",
      "features in the blue area: [10, 141, 144, 173]\n"
     ]
    }
   ],
   "source": [
    "# over 170 features\n",
    "\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest = RandomForestRegressor(\n",
    "   n_jobs = -1, \n",
    "   max_depth = 5\n",
    ")\n",
    "\n",
    "xgb_reg = XGBRegressor(tree_method = \"gpu_hist\",single_precision_histogram=True, gpu_id=0)\n",
    "\n",
    "\n",
    "boruta = BorutaPy(\n",
    "   estimator = xgb_reg, \n",
    "   n_estimators = 'auto',\n",
    "   max_iter = 100 # number of trials to perform\n",
    ")\n",
    "\n",
    "boruta.fit(X_train.values, y_train)\n",
    "\n",
    "green_area = X_train.columns[boruta.support_].to_list()\n",
    "blue_area = X_train.columns[boruta.support_weak_].to_list()\n",
    "print('features in the green area:', green_area)\n",
    "print('features in the blue area:', blue_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ac2c8b3-b0b0-4e51-855b-ce882e42658e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_keep = X_train.columns[boruta.support_].to_list()\n",
    "len(cols_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bddc9308-778b-40ed-8d88-a1b9a1974d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train[cols_to_keep]).to_csv('./data/datatset2.csv')\n",
    "pd.DataFrame(X_test[cols_to_keep]).to_csv('./data/datatset2_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9773f007-e621-43f1-992d-f4cf4a678bc6",
   "metadata": {},
   "source": [
    "## Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4fa6e999-95d5-4b78-90cd-67625e0f56f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_greedy = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fcc218a9-18b6-4e2c-87d9-b61196ce66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clothing_data_df.drop('price', axis=1)\n",
    "y = clothing_data_df['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(np.hstack((X_train['page 2 (clothing model)'], X_test['page 2 (clothing model)'])))\n",
    "X_train['page 2 (clothing model)'] = le.transform(X_train['page 2 (clothing model)'])\n",
    "X_test['page 2 (clothing model)'] = le.transform(X_test['page 2 (clothing model)'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9945a6cb-1cae-485a-856a-fdbf55b99756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn import linear_model \n",
    "from sklearn.model_selection import cross_validate as cross_validation\n",
    "from scipy import sparse\n",
    "from itertools import combinations\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a865cf3a-886e-4744-8094-ea559ca3ab51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using Amazon Access greedy feature creation\n",
    "\n",
    "def group_data(data, degree=3, hash=hash):\n",
    "    new_data = []\n",
    "    m, n = data.shape\n",
    "    for indices in combinations(range(n), degree):\n",
    "        new_data.append([hash(tuple(v)) for v in data.values[:, indices]])\n",
    "    return np.array(new_data).T\n",
    "\n",
    "def OneHotEncoder(data, keymap=None):\n",
    "    \"\"\"\n",
    "    OneHotEncoder takes data matrix with categorical columns and\n",
    "    converts it to a sparse binary matrix.\n",
    "\n",
    "    Returns sparse binary matrix and keymap mapping categories to indicies.\n",
    "    \"\"\"\n",
    "    if keymap is None:\n",
    "        keymap = []\n",
    "        for col in data.T:\n",
    "            uniques = set(list(col))\n",
    "            keymap.append(dict((key, i) for i, key in enumerate(uniques)))\n",
    "    total_pts = data.shape[0]\n",
    "    outdat = []\n",
    "    for i, col in enumerate(data.T):\n",
    "        km = keymap[i]\n",
    "        num_labels = len(km)\n",
    "        spmat = sparse.lil_matrix((total_pts, num_labels))\n",
    "        for j, val in enumerate(col):\n",
    "            if val in km:\n",
    "                spmat[j, km[val]] = 1\n",
    "        outdat.append(spmat)\n",
    "    outdat = sparse.hstack(outdat).tocsr()\n",
    "    return outdat\n",
    "    \n",
    "def cv_loop(X, y, model, N, print_score=False):\n",
    "    mean_auc = 0.\n",
    "    for i in range(N):\n",
    "        X_train, X_cv, y_train, y_cv = train_test_split(X, y, test_size=.20)\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict_proba(X_cv)#[:, 1]\n",
    "        auc = metrics.roc_auc_score(y_cv, preds, multi_class='ovr')\n",
    "        if print_score:\n",
    "            print (\"AUC (fold %d/%d): %f\" % (i + 1, N, auc))\n",
    "        mean_auc += auc\n",
    "    return mean_auc/N    \n",
    "    \n",
    "num_train = X_train.shape[0]\n",
    "\n",
    "# Transform data\n",
    "dp = group_data(X, degree=2)\n",
    "dt = group_data(X, degree=3)\n",
    "\n",
    "\n",
    "X_2 = dp[:num_train]\n",
    "X_3 = dt[:num_train]\n",
    "\n",
    "X_test = X[num_train:]\n",
    "X = X_train\n",
    "\n",
    "X_test_2 = dp[num_train:]\n",
    "X_test_3 = dt[num_train:]\n",
    "\n",
    "X_train_all = np.hstack((X, X_2, X_3))\n",
    "X_test_all = np.hstack((X_test, X_test_2, X_test_3))\n",
    "num_features = X_train_all.shape[1]\n",
    "\n",
    "model = linear_model.LogisticRegression()\n",
    "\n",
    "# Xts holds one hot encodings for each individual feature in memory\n",
    "# speeding up feature selection\n",
    "Xts = [OneHotEncoder(X_train_all[:, [i]]) for i in range(num_features)]\n",
    "\n",
    "## greedy feature selection\n",
    "score_hist = []\n",
    "N = 10\n",
    "# Greedy feature selection loop\n",
    "good_features = set([124,193,217,6])\n",
    "if search_greedy:\n",
    "    while len(score_hist) < 2 or score_hist[-1][0] > score_hist[-2][0]: #keep in loop if less than two features, stop criteria is when mean auc score starts to drop\n",
    "        scores = []\n",
    "        for f in np.random.choice(range(len(Xts)), size=6, replace=False): #added sampling to randomize and speed up results. from past runs it seems like there is a high number of high-scoring features and each run takes long. 6 is enough.\n",
    "            if f not in good_features:\n",
    "                feats = list(good_features) + [f]\n",
    "                Xt = sparse.hstack([Xts[j] for j in feats]).tocsr()\n",
    "                score = cv_loop(Xt, y_train, model, N)\n",
    "                scores.append((score, f))\n",
    "                print( \"Feature: %i Mean AUC: %f\" % (f, score))\n",
    "        good_features.add(sorted(scores)[-2][1]) #unlikely that there will be a mean auc of 1 but if there is, choose the second largest. \n",
    "        score_hist.append(sorted(scores)[-2])\n",
    "        print (\"Current features: %s\" % sorted(list(good_features)))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b98cffed-eb95-4511-b224-47aeb2a20d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if search_greedy:\n",
    "    # try selecting all features of mean AUC 1\n",
    "    feature_mean_auc_scores = []\n",
    "    for f in range(len(Xts)): \n",
    "        Xt = sparse.hstack([Xts[f]]).tocsr()\n",
    "        score = cv_loop(Xt, y_train, model, N)\n",
    "        feature_mean_auc_scores.append((score, f))\n",
    "        print( \"Feature: %i Mean AUC: %f\" % (f, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1d482fd-27fb-422a-9647-11e8ee89abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_features = [6,16,25,33,40,51,52,53,56,57,58,59,60,62,70,78,85,96,97,98,101,102,103,104,105,107,109,121,132,133,134,137,138,139,140,141,143,149,160,161,162,165,166,167,168,169,171,181,182,183,186,187,188,189,190,192,194,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230]\n",
    "if search_greedy:\n",
    "    good_features = [x[1] for x in feature_mean_auc_scores if x[0] > 0.9]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2eeb22-c479-45a0-bf3e-7e7ce3381fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23b72958-ecae-4ad1-b46a-aed2d00d6f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove last added feature from good_features\n",
    "#good_features.remove(score_hist[-1][1])\n",
    "#good_features = sorted(list(good_features))\n",
    "\n",
    "X_train = X_train_all[:, good_features]\n",
    "X_test = X_test_all[:, good_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a6a3b20b-b957-4f7e-8e40-f0bd39c4b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train).to_csv('./data/datatset3.csv')\n",
    "pd.DataFrame(X_test).to_csv('./data/datatset3_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a6bf81-3bd5-40d6-96c9-7dc3c17ba171",
   "metadata": {},
   "source": [
    "# Dataset 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c95c1a2-40c9-4e12-a625-e067cb0dc49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clothing_data_df.drop('price', axis=1)\n",
    "y = clothing_data_df['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(np.hstack((X_train['page 2 (clothing model)'], X_test['page 2 (clothing model)'])))\n",
    "X_train['page 2 (clothing model)'] = le.transform(X_train['page 2 (clothing model)'])\n",
    "X_test['page 2 (clothing model)'] = le.transform(X_test['page 2 (clothing model)'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "83361635-dcbb-4778-bb2f-8cb0b13c3ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(data, degree=2, operation=None):\n",
    "    new_data = []\n",
    "    m, n = data.shape\n",
    "    for indices in combinations(range(n), degree):\n",
    "        if operation=='addition':\n",
    "            new_data.append([np.sum(tuple(v)) for v in data.values[:, indices]])\n",
    "        elif operation=='subtraction':\n",
    "            new_data.append([(v[0]-v[1]) for v in data.values[:, indices]])\n",
    "        elif operation=='multiplication':\n",
    "            new_data.append([(v[0]*v[1]) for v in data.values[:, indices]])\n",
    "        elif operation=='division':\n",
    "            new_data.append([(v[0]/v[1]) for v in data.values[:, indices]])\n",
    "    return np.array(new_data).T\n",
    "\n",
    "operations=['addition','subtraction','multiplication','division']\n",
    "golden_features=combine_features(X_train, operation=operations[0])\n",
    "for op in operations[1:]:\n",
    "    golden_features=np.hstack((golden_features,combine_features(X_train, operation=op)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "771f103e-bccc-4034-97ff-8a45094d1ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132379, 220)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golden_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5be5473b-3d4b-4bc9-b8b0-1ce7fd423913",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_w_inf = pd.DataFrame(golden_features).apply(np.isinf).sum()[pd.DataFrame(golden_features).apply(np.isinf).sum()>0].index\n",
    "golden_features = pd.DataFrame(golden_features)\n",
    "for f in cols_w_inf:\n",
    "    golden_features[f] = golden_features[f].apply(lambda row: 9999 if np.isinf(row) else row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237b1283-fdd5-4c60-ae6a-44771286ea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 4: run Boruta feature selection algo for 'golden features' https://mljar.com/automated-machine-learning/golden-features/#:~:text=Golden%20Featues%20Generation%20Overview,power%20of%20newly%20created%20features.\n",
    "# reall great explanation https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a\n",
    "\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "boruta = BorutaPy(\n",
    "   estimator = xgb_reg, \n",
    "   n_estimators = 'auto',\n",
    "   max_iter = 100 # number of trials to perform\n",
    ")\n",
    "\n",
    "boruta.fit(golden_features.values, y_train)\n",
    "\n",
    "green_area = X_train.columns[boruta.support_].to_list()\n",
    "blue_area = X_train.columns[boruta.support_weak_].to_list()\n",
    "print('features in the green area:', green_area)\n",
    "print('features in the blue area:', blue_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c1e1dd-1802-4f6d-98c6-bf7e79f38f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset4 = \n",
    "golden_features[green_area]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ad866-30b2-44b0-83db-85da926c18b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train).to_csv('./data/datatset4.csv')\n",
    "pd.DataFrame(X_test).to_csv('./data/datatset4_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc5382f-e136-44bf-bdec-a9487223e34e",
   "metadata": {},
   "source": [
    "# Dataset 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f48477f3-0d62-47df-910a-d53de9e8de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 5: original\n",
    "X = clothing_data_df.drop('price', axis=1)\n",
    "y = clothing_data_df['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(np.hstack((X_train['page 2 (clothing model)'], X_test['page 2 (clothing model)'])))\n",
    "X_train['page 2 (clothing model)'] = le.transform(X_train['page 2 (clothing model)'])\n",
    "X_test['page 2 (clothing model)'] = le.transform(X_test['page 2 (clothing model)'])\n",
    "pd.DataFrame(X_train).to_csv('./data/datatset5.csv')\n",
    "pd.DataFrame(X_test).to_csv('./data/datatset5_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b6ac8-48fd-4b6f-a696-7c991fc97de9",
   "metadata": {},
   "source": [
    "# Param tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772bbb82-d328-4f5f-9449-d84ec305066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#models = [model,ds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e3d307-db3c-4b8a-932b-1161e382e305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8178bcde-bcc3-4bbc-bbf9-cbf6e22c134e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1b3b7f-e388-4813-ac73-aa4cde8da905",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1454a346-a61a-43b4-b25d-872d7bc83305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e85b265-1d99-47bf-92c2-ea50a8b575ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6dae19-8c2e-4c1b-8b2c-2d511d66a461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd938a82-260d-4e74-88bf-8e2c6553adcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b88d8e-093f-42cd-b0e7-b7fdf1276c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfe2d37-f603-4f3d-86eb-e523973f0124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357dfc79-5a12-42b8-a015-d5c206cb205b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b70e896-0634-4e7b-99d6-8291802a1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns = ['model','rmse','r2','mape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18800b3c-9a73-4644-a724-704ff6bc81a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_levels = round(clothing_data_df.columns.shape[0]*0.5)\n",
    "categorical_features = clothing_data_df.select_dtypes(exclude=[np.number]).columns\n",
    "cats_many = []\n",
    "cats_few = []\n",
    "for ft in categorical_features:\n",
    "    levels = clothing_data_df[ft].unique().shape[0]\n",
    "    if levels > max_levels:\n",
    "        cats_many.append(ft)\n",
    "    else:\n",
    "        cats_few.append(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446fd892-7440-4021-8403-73a8d782e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = clothing_data_df.select_dtypes([np.number]).drop(['price'], axis=1).columns\n",
    "numeric_features\n",
    "\n",
    "categorical_features = clothing_data_df.select_dtypes(exclude=[np.number]).columns\n",
    "categorical_features\n",
    "\n",
    "X = clothing_data_df.drop('price', axis=1)\n",
    "y = clothing_data_df['price']\n",
    "\n",
    "y = np.log(y)\n",
    "\n",
    "X_train, X_test_tmp, y_train, y_test_tmp = train_test_split(X, y, test_size=0.3)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_tmp, y_test_tmp, test_size=0.5)\n",
    "\n",
    "del X_test_tmp, y_test_tmp\n",
    "\n",
    "selected_model = XGBRegressor(tree_method = \"gpu_hist\",single_precision_histogram=True, gpu_id=0)\n",
    "\n",
    "\n",
    "categorical_transformer_many_level = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('encoder', encoders['BackwardDifferenceEncoder']())\n",
    "    ]\n",
    ")    \n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('encoder', encoders['OneHotEncoder']())\n",
    "    ]\n",
    ") \n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numerical', numeric_transformer, numeric_features),\n",
    "        ('categorical_many', categorical_transformer_many_level, cats_many),\n",
    "        ('categorical', categorical_transformer, cats_few)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', selected_model)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30738153-6650-41c5-8a8f-7de978bd3e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe.named_steps['preprocessor'].fit(X_train)\n",
    "\n",
    "    \n",
    "numeric_feat_names = pipe.named_steps['preprocessor'].transformers_[0][2]\n",
    "cat_feat_names = pipe.named_steps['preprocessor'].transformers_[1][2]\n",
    "#redo_names = pipe.named_steps['preprocessor'].transformers_[2][1].named_steps['encoder'].get_feature_names()\n",
    "#base_names = pipe.named_steps['preprocessor'].transformers_[2][2]   \n",
    "#one_hot_feat_names=[]\n",
    "#for i in range(len(base_names)):\n",
    "#    one_hot_feat_names.append([base_names[i]+'_'+x.split('_')[-1] for x in redo_names if x[0] == str(i)])\n",
    "    \n",
    "feature_names = list(numeric_feat_names) + list(cat_feat_names) #+ [y for x in one_hot_feat_names for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e011dd-0dd4-41b1-87fd-68e8ed483ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
