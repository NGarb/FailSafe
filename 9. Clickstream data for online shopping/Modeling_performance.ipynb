{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5846da79-9d1a-42ef-a904-11a36b8b7087",
   "metadata": {},
   "source": [
    "## Modeling for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d03dd50-ea4f-49d1-8a0e-b4b847f39714",
   "metadata": {},
   "source": [
    "### implementing some learnings from Amazon Access (project 11)\n",
    "- generating multiple datasets \n",
    "- zipping datasets with models \n",
    "- stacking models for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e9ba757-efd3-47e2-a50b-e2ee6f2322eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn import metrics, linear_model, ensemble\n",
    "from yellowbrick.regressor import residuals_plot, prediction_error\n",
    "from fast_ml.model_development import train_valid_test_split\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from tpot import TPOTRegressor\n",
    "import category_encoders as ce\n",
    "import time\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "encoders = {\n",
    "\n",
    "    'BackwardDifferenceEncoder': ce.backward_difference.BackwardDifferenceEncoder,\n",
    "    'OneHotEncoder': ce.one_hot.OneHotEncoder,\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dbb945a-a221-46d2-98c0-3ee2343861b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('..')\n",
    "from Utils.Metrics import regression as reg_metrics\n",
    "os.chdir('./9. Clickstream data for online shopping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f630143b-9aff-47ee-aab1-fc1a78fe4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "clothing_data_df = pd.read_csv('./data/e-shop data and description/e-shop clothing 2008.csv',sep=',').drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "be9fbfbf-cd62-4de3-b486-ba42e02bce94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "35d01c53-f86f-474c-b621-c54be684c52d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d08b2e09-e858-46c2-960b-499ac035327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_models = [\n",
    "    \"LR:dataset1\",\n",
    "    \"LR:dataset2\",\n",
    "    \"LR:dataset3\",\n",
    "    \"LR:dataset4\",\n",
    "    \"RFC:dataset1\",\n",
    "    \"RFC:dataset2\",\n",
    "    \"RFC:dataset3\",\n",
    "    \"RFC:dataset4\",\n",
    "    \"GBC:dataset1\",\n",
    "    \"GBC:dataset2\",\n",
    "    \"GBC:dataset3\",\n",
    "    \"GBC:dataset4\"\n",
    "]\n",
    "\n",
    "models = []\n",
    "for item in selected_models:\n",
    "    model_id, dataset = item.split(':')\n",
    "    # instantiating models (with datasets)\n",
    "    model = {'LR': linear_model.LogisticRegression,\n",
    "             'GBC': ensemble.GradientBoostingClassifier,\n",
    "             'RFC': ensemble.RandomForestClassifier,\n",
    "             'ETC': ensemble.ExtraTreesClassifier}[model_id]() \n",
    "    model.set_params()\n",
    "    models.append((model, dataset))\n",
    "\n",
    "datasets = [dataset for model, dataset in models]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b225a5-a5c9-4b83-a594-1057bbdbafd6",
   "metadata": {},
   "source": [
    "## Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cdf397-623a-46ba-92bd-fe015af84246",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clothing_data_df.drop('price', axis=1)\n",
    "y = clothing_data_df['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6bed3eb4-0ee2-46e6-a461-5ee76e71af19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset 1\n",
    "# Create a dataset where the features are the effects of a logistic regression trained on sparsified data.\n",
    "# Essentially what this would give you is the exact effect of the value of a feature on the target. would work if target is binary but in my case there are twenty unique values\n",
    "# redefine target to <= 40 or > 40\n",
    "y = (y <= 40).astype(int)\n",
    "\n",
    "def sparsify(X, X_test):\n",
    "    \"\"\"Return One-Hot encoded datasets.\"\"\"\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(np.vstack((X, X_test)))\n",
    "    return enc.transform(X.values), enc.transform(X_test.values)\n",
    "\n",
    "from sklearn import linear_model \n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "Xe_train = np.zeros(X_train.shape)\n",
    "Xe_test = np.zeros(X_test.shape)\n",
    "n_cols = Xe_train.shape[1]\n",
    "\n",
    "model = linear_model.LogisticRegression(C=2)\n",
    "X_train, X_test = sparsify(X_train, X_test)\n",
    "\n",
    "kfold = KFold(5)\n",
    "\n",
    "# alrighty. so what's happening is unrolling of all unique values in all columns, for each of these columns, training a logistic model and then slotting those values (coeffs) back into the unrolled spaces\n",
    "for train, cv in kfold.split(X_train):\n",
    "    model.fit(X_train[train], y[train])\n",
    "    colindices = X_test.nonzero()[1] #flattened non zero col indices #remember the data has been sparsified so 14501 unique col indices and 364045 non unique\n",
    "    for i, k in zip(cv, range(len(cv))): # i - row number, k - row index\n",
    "        for j in range(n_cols): #n_cols is 11 (pre sparsify)\n",
    "            z = colindices[n_cols*k + j]\n",
    "            Xe_train[i, j] = model.coef_[0, z]\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "colindices = X_test.nonzero()[1]\n",
    "for i in range(Xe_test.shape[0]):\n",
    "    for j in range(n_cols):\n",
    "        z = colindices[n_cols*i + j]\n",
    "        Xe_test[i, j] = model.coef_[0, z]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8353a7c-ab31-419c-b402-89910cf3bce7",
   "metadata": {},
   "source": [
    "## Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b4b0a703-2143-44ec-ae80-0faaada40318",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clothing_data_df.drop('price', axis=1)\n",
    "y = clothing_data_df['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c786f069-0ec8-47a9-b12a-854ab07c4a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with polynomial features \n",
    "    \n",
    "# ordinal encode \n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(np.hstack((X_train['page 2 (clothing model)'], X_test['page 2 (clothing model)'])))\n",
    "X_train['page 2 (clothing model)'] = le.transform(X_train['page 2 (clothing model)'])\n",
    "X_test['page 2 (clothing model)'] = le.transform(X_test['page 2 (clothing model)'])\n",
    "    \n",
    "X_train = list(X_train.T.values)\n",
    "X_test = list(X_test.T.values)\n",
    "X_train = [list(feature) for feature in X_train]\n",
    "X_test = [list(feature) for feature in X_test]\n",
    "\n",
    "n_features = len(X_train)    \n",
    "    \n",
    "for i in range(n_features):\n",
    "    #for j in range(1): j = 0 ? ratio of all features with first one? is it maybe meant to be range(i+1,n_features)? \n",
    "    for j in range(i+1,n_features):\n",
    "        X_train.append([round(a/(b + 1), 3) for a, b in zip(X_train[i], X_train[j])])\n",
    "        X_test.append([round(a/(b + 1), 3) for a, b in zip(X_test[i], X_test[j])])\n",
    "\n",
    "        X_train.append([round(a/(b + 1), 3) for a, b in zip(X_train[j], X_train[i])])\n",
    "        X_test.append([round(a/(b + 1), 3) for a, b in zip(X_test[j], X_test[i])])\n",
    "\n",
    "        X_train.append([a*b for a, b in zip(X_train[j], X_train[i])])\n",
    "        X_test.append([a*b for a, b in zip(X_test[j], X_test[i])])\n",
    "\n",
    "# remove constant features\n",
    "for i in range(len(X_train) - 1, -1, -1):\n",
    "    if np.var(X_train[i]) + np.var(X_test[i]) == 0:\n",
    "        X_train.pop(i)\n",
    "        X_test.pop(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9773f007-e621-43f1-992d-f4cf4a678bc6",
   "metadata": {},
   "source": [
    "## Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc218a9-18b6-4e2c-87d9-b61196ce66d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clothing_data_df.drop('price', axis=1)\n",
    "y = clothing_data_df['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a865cf3a-886e-4744-8094-ea559ca3ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Amazon Access greedy feature creation\n",
    "\n",
    "def group_data(data, degree=3, hash=hash):\n",
    "    new_data = []\n",
    "    m, n = data.shape\n",
    "    for indices in combinations(range(n), degree):\n",
    "        new_data.append([hash(tuple(v)) for v in data[:, indices]])\n",
    "    return np.array(new_data).T\n",
    "    \n",
    "num_train = X_train.shape[0]\n",
    "\n",
    "# Transform data\n",
    "print (\"Transforming data...\")\n",
    "dp = group_data(X, degree=2)\n",
    "dt = group_data(X, degree=3)\n",
    "\n",
    "X = X_train\n",
    "X_2 = dp[:num_train]\n",
    "X_3 = dt[:num_train]\n",
    "\n",
    "X_test = all_data[num_train:]\n",
    "X_test_2 = dp[num_train:]\n",
    "X_test_3 = dt[num_train:]\n",
    "\n",
    "X_train_all = np.hstack((X, X_2, X_3))\n",
    "X_test_all = np.hstack((X_test, X_test_2, X_test_3))\n",
    "num_features = X_train_all.shape[1]\n",
    "\n",
    "model = linear_model.LogisticRegression()\n",
    "\n",
    "# Xts holds one hot encodings for each individual feature in memory\n",
    "# speeding up feature selection\n",
    "Xts = [OneHotEncoder(X_train_all[:, [i]])[0] for i in range(num_features)]\n",
    "\n",
    "print (\"Performing greedy feature selection...\")\n",
    "score_hist = []\n",
    "N = 10\n",
    "good_features_list = [\n",
    "    [0, 8, 9, 10, 19, 34, 36, 37, 38, 41, 42, 43, 47, 53, 55,\n",
    "     60, 61, 63, 64, 67, 69, 71, 75, 81, 82, 85],\n",
    "    [0, 1, 7, 8, 9, 10, 36, 37, 38, 41, 42, 43, 47, 51, 53,\n",
    "     56, 60, 61, 63, 64, 66, 67, 69, 71, 75, 79, 85, 91],\n",
    "    [0, 7, 9, 24, 36, 37, 41, 42, 47, 53, 61, 63, 64, 67, 69, 71, 75, 85],\n",
    "    [0, 7, 9, 20, 36, 37, 38, 41, 42, 45, 47,\n",
    "     53, 60, 63, 64, 67, 69, 71, 81, 85, 86]\n",
    "]\n",
    "\n",
    "# Greedy feature selection loop\n",
    "if not good_features_list:\n",
    "    good_features = set([])\n",
    "    while len(score_hist) < 2 or score_hist[-1][0] > score_hist[-2][0]:\n",
    "        scores = []\n",
    "        for f in range(len(Xts)):\n",
    "            if f not in good_features:\n",
    "                feats = list(good_features) + [f]\n",
    "                Xt = sparse.hstack([Xts[j] for j in feats]).tocsr()\n",
    "                score = cv_loop(Xt, y, model, N)\n",
    "                scores.append((score, f))\n",
    "                print( \"Feature: %i Mean AUC: %f\" % (f, score))\n",
    "        good_features.add(sorted(scores)[-1][1])\n",
    "        score_hist.append(sorted(scores)[-1])\n",
    "        print (\"Current features: %s\" % sorted(list(good_features)))\n",
    "\n",
    "    # Remove last added feature from good_features\n",
    "    good_features.remove(score_hist[-1][1])\n",
    "    good_features = sorted(list(good_features))\n",
    "\n",
    "for i, good_features in enumerate(good_features_list):\n",
    "    suffix = str(i + 1) if i else ''\n",
    "    Xt = np.vstack((X_train_all[:, good_features],\n",
    "                    X_test_all[:, good_features]))\n",
    "    X_train = Xt[:num_train]\n",
    "    X_test = Xt[num_train:]\n",
    "    data.save_dataset(\"greedy%s\" % suffix, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37649aa-2a9c-4823-be11-d41af1185d09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd938a82-260d-4e74-88bf-8e2c6553adcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b88d8e-093f-42cd-b0e7-b7fdf1276c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfe2d37-f603-4f3d-86eb-e523973f0124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357dfc79-5a12-42b8-a015-d5c206cb205b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b70e896-0634-4e7b-99d6-8291802a1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns = ['model','rmse','r2','mape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18800b3c-9a73-4644-a724-704ff6bc81a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_levels = round(clothing_data_df.columns.shape[0]*0.5)\n",
    "categorical_features = clothing_data_df.select_dtypes(exclude=[np.number]).columns\n",
    "cats_many = []\n",
    "cats_few = []\n",
    "for ft in categorical_features:\n",
    "    levels = clothing_data_df[ft].unique().shape[0]\n",
    "    if levels > max_levels:\n",
    "        cats_many.append(ft)\n",
    "    else:\n",
    "        cats_few.append(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446fd892-7440-4021-8403-73a8d782e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = clothing_data_df.select_dtypes([np.number]).drop(['price'], axis=1).columns\n",
    "numeric_features\n",
    "\n",
    "categorical_features = clothing_data_df.select_dtypes(exclude=[np.number]).columns\n",
    "categorical_features\n",
    "\n",
    "X = clothing_data_df.drop('price', axis=1)\n",
    "y = clothing_data_df['price']\n",
    "\n",
    "y = np.log(y)\n",
    "\n",
    "X_train, X_test_tmp, y_train, y_test_tmp = train_test_split(X, y, test_size=0.3)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_tmp, y_test_tmp, test_size=0.5)\n",
    "\n",
    "del X_test_tmp, y_test_tmp\n",
    "\n",
    "selected_model = XGBRegressor(tree_method = \"gpu_hist\",single_precision_histogram=True, gpu_id=0)\n",
    "\n",
    "\n",
    "categorical_transformer_many_level = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('encoder', encoders['BackwardDifferenceEncoder']())\n",
    "    ]\n",
    ")    \n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('encoder', encoders['OneHotEncoder']())\n",
    "    ]\n",
    ") \n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numerical', numeric_transformer, numeric_features),\n",
    "        ('categorical_many', categorical_transformer_many_level, cats_many),\n",
    "        ('categorical', categorical_transformer, cats_few)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', selected_model)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30738153-6650-41c5-8a8f-7de978bd3e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe.named_steps['preprocessor'].fit(X_train)\n",
    "\n",
    "    \n",
    "numeric_feat_names = pipe.named_steps['preprocessor'].transformers_[0][2]\n",
    "cat_feat_names = pipe.named_steps['preprocessor'].transformers_[1][2]\n",
    "#redo_names = pipe.named_steps['preprocessor'].transformers_[2][1].named_steps['encoder'].get_feature_names()\n",
    "#base_names = pipe.named_steps['preprocessor'].transformers_[2][2]   \n",
    "#one_hot_feat_names=[]\n",
    "#for i in range(len(base_names)):\n",
    "#    one_hot_feat_names.append([base_names[i]+'_'+x.split('_')[-1] for x in redo_names if x[0] == str(i)])\n",
    "    \n",
    "feature_names = list(numeric_feat_names) + list(cat_feat_names) #+ [y for x in one_hot_feat_names for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e011dd-0dd4-41b1-87fd-68e8ed483ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
