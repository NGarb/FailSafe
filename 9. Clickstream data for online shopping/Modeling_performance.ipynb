{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5846da79-9d1a-42ef-a904-11a36b8b7087",
   "metadata": {},
   "source": [
    "## Modeling for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d03dd50-ea4f-49d1-8a0e-b4b847f39714",
   "metadata": {},
   "source": [
    "### implementing some learnings from Amazon Access (project 11)\n",
    "- generating multiple datasets \n",
    "- zipping datasets with models \n",
    "- stacking models for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e9ba757-efd3-47e2-a50b-e2ee6f2322eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn import metrics, linear_model, ensemble\n",
    "from yellowbrick.regressor import residuals_plot, prediction_error\n",
    "from fast_ml.model_development import train_valid_test_split\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from tpot import TPOTRegressor\n",
    "import category_encoders as ce\n",
    "import time\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "encoders = {\n",
    "\n",
    "    'BackwardDifferenceEncoder': ce.backward_difference.BackwardDifferenceEncoder,\n",
    "    'OneHotEncoder': ce.one_hot.OneHotEncoder,\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dbb945a-a221-46d2-98c0-3ee2343861b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('..')\n",
    "from Utils.Metrics import regression as reg_metrics\n",
    "os.chdir('./9. Clickstream data for online shopping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f630143b-9aff-47ee-aab1-fc1a78fe4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "clothing_data_df = pd.read_csv('./data/e-shop data and description/e-shop clothing 2008.csv',sep=',').drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be9fbfbf-cd62-4de3-b486-ba42e02bce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clothing_data_df.drop('price', axis=1)\n",
    "y = clothing_data_df['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d08b2e09-e858-46c2-960b-499ac035327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_models = [\n",
    "    \"LR:dataset1\",\n",
    "    \"LR:dataset2\",\n",
    "    \"LR:dataset3\",\n",
    "    \"LR:dataset4\",\n",
    "    \"RFC:dataset1\",\n",
    "    \"RFC:dataset2\",\n",
    "    \"RFC:dataset3\",\n",
    "    \"RFC:dataset4\",\n",
    "    \"GBC:dataset1\",\n",
    "    \"GBC:dataset2\",\n",
    "    \"GBC:dataset3\",\n",
    "    \"GBC:dataset4\"\n",
    "]\n",
    "\n",
    "models = []\n",
    "for item in selected_models:\n",
    "    model_id, dataset = item.split(':')\n",
    "    # instantiating models (with datasets)\n",
    "    model = {'LR': linear_model.LogisticRegression,\n",
    "             'GBC': ensemble.GradientBoostingClassifier,\n",
    "             'RFC': ensemble.RandomForestClassifier,\n",
    "             'ETC': ensemble.ExtraTreesClassifier}[model_id]() #I have never seen this done before.\n",
    "    model.set_params()\n",
    "    models.append((model, dataset))\n",
    "\n",
    "datasets = [dataset for model, dataset in models]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e93dcc5c-f4a9-419d-a457-00618095ba77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     1     2 ... 26473 26474 26475]\n"
     ]
    }
   ],
   "source": [
    "for train, cv in kfold.split(X_train):\n",
    "    print(cv)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6bed3eb4-0ee2-46e6-a461-5ee76e71af19",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [132379, 165474]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16924/1633684987.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mXe_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[0mcolindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXe_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1506\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1508\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m   1509\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1510\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    979\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\AutoML\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    333\u001b[0m             \u001b[1;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;33m%\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [132379, 165474]"
     ]
    }
   ],
   "source": [
    "# dataset 1\n",
    "# Create a dataset where the features are the effects of a logistic regression trained on sparsified data.\n",
    "def sparsify(X, X_test):\n",
    "    \"\"\"Return One-Hot encoded datasets.\"\"\"\n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(np.vstack((X, X_test)))\n",
    "    return enc.transform(X.values), enc.transform(X_test.values)\n",
    "\n",
    "from sklearn import linear_model \n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "Xe_train = np.zeros(X_train.shape)\n",
    "Xe_test = np.zeros(X_test.shape)\n",
    "n_cols = Xe_train.shape[1]\n",
    "\n",
    "model = linear_model.LogisticRegression(C=2)\n",
    "X_train, X_test = sparsify(X_train, X_test)\n",
    "\n",
    "kfold = KFold(5)\n",
    "\n",
    "for train, cv in kfold.split(X_train):\n",
    "    model.fit(X_train[train], y[train])\n",
    "    colindices = X_test.nonzero()[1]\n",
    "    for i, k in zip(cv, range(len(cv))):\n",
    "        for j in range(n_cols):\n",
    "            z = colindices[n_cols*k + j]\n",
    "            Xe_train[i, j] = model.coef_[0, z]\n",
    "\n",
    "model.fit(X_train, y)\n",
    "colindices = X_test.nonzero()[1]\n",
    "for i in range(Xe_test.shape[0]):\n",
    "    for j in range(n_cols):\n",
    "        z = colindices[n_cols*i + j]\n",
    "        Xe_test[i, j] = model.coef_[0, z]\n",
    "\n",
    "return Xe_train, Xe_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c786f069-0ec8-47a9-b12a-854ab07c4a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6907d89f-7c89-43e5-9bd5-5f78fefddd59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a865cf3a-886e-4744-8094-ea559ca3ab51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37649aa-2a9c-4823-be11-d41af1185d09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd938a82-260d-4e74-88bf-8e2c6553adcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b88d8e-093f-42cd-b0e7-b7fdf1276c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfe2d37-f603-4f3d-86eb-e523973f0124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357dfc79-5a12-42b8-a015-d5c206cb205b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b70e896-0634-4e7b-99d6-8291802a1cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(columns = ['model','rmse','r2','mape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18800b3c-9a73-4644-a724-704ff6bc81a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_levels = round(clothing_data_df.columns.shape[0]*0.5)\n",
    "categorical_features = clothing_data_df.select_dtypes(exclude=[np.number]).columns\n",
    "cats_many = []\n",
    "cats_few = []\n",
    "for ft in categorical_features:\n",
    "    levels = clothing_data_df[ft].unique().shape[0]\n",
    "    if levels > max_levels:\n",
    "        cats_many.append(ft)\n",
    "    else:\n",
    "        cats_few.append(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446fd892-7440-4021-8403-73a8d782e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = clothing_data_df.select_dtypes([np.number]).drop(['price'], axis=1).columns\n",
    "numeric_features\n",
    "\n",
    "categorical_features = clothing_data_df.select_dtypes(exclude=[np.number]).columns\n",
    "categorical_features\n",
    "\n",
    "X = clothing_data_df.drop('price', axis=1)\n",
    "y = clothing_data_df['price']\n",
    "\n",
    "y = np.log(y)\n",
    "\n",
    "X_train, X_test_tmp, y_train, y_test_tmp = train_test_split(X, y, test_size=0.3)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_tmp, y_test_tmp, test_size=0.5)\n",
    "\n",
    "del X_test_tmp, y_test_tmp\n",
    "\n",
    "selected_model = XGBRegressor(tree_method = \"gpu_hist\",single_precision_histogram=True, gpu_id=0)\n",
    "\n",
    "\n",
    "categorical_transformer_many_level = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('encoder', encoders['BackwardDifferenceEncoder']())\n",
    "    ]\n",
    ")    \n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('encoder', encoders['OneHotEncoder']())\n",
    "    ]\n",
    ") \n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('numerical', numeric_transformer, numeric_features),\n",
    "        ('categorical_many', categorical_transformer_many_level, cats_many),\n",
    "        ('categorical', categorical_transformer, cats_few)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', selected_model)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30738153-6650-41c5-8a8f-7de978bd3e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe.named_steps['preprocessor'].fit(X_train)\n",
    "\n",
    "    \n",
    "numeric_feat_names = pipe.named_steps['preprocessor'].transformers_[0][2]\n",
    "cat_feat_names = pipe.named_steps['preprocessor'].transformers_[1][2]\n",
    "#redo_names = pipe.named_steps['preprocessor'].transformers_[2][1].named_steps['encoder'].get_feature_names()\n",
    "#base_names = pipe.named_steps['preprocessor'].transformers_[2][2]   \n",
    "#one_hot_feat_names=[]\n",
    "#for i in range(len(base_names)):\n",
    "#    one_hot_feat_names.append([base_names[i]+'_'+x.split('_')[-1] for x in redo_names if x[0] == str(i)])\n",
    "    \n",
    "feature_names = list(numeric_feat_names) + list(cat_feat_names) #+ [y for x in one_hot_feat_names for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e011dd-0dd4-41b1-87fd-68e8ed483ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
